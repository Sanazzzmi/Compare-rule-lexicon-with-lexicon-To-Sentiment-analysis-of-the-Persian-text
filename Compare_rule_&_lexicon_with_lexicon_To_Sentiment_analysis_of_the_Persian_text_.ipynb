{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Compare rule & lexicon with lexicon To Sentiment analysis of the Persian text .ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9D-v9dKeydfB"
      ],
      "mount_file_id": "114GUuSUPbs3df6_CP-zO_grXK7Sknesm",
      "authorship_tag": "ABX9TyOiVZmvoUtb5ZqTzCd1Hdxk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sanazzzmi/Compare-rule-lexicon-with-lexicon-To-Sentiment-analysis-of-the-Persian-text/blob/main/Compare_rule_%26_lexicon_with_lexicon_To_Sentiment_analysis_of_the_Persian_text_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### install and import data"
      ],
      "metadata": {
        "id": "PJ6eB17kxe6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "download lexicon url\n",
        "https://github.com/Text-Mining/Persian-Sentiment-Resources/blob/master/PerSent.xlsx"
      ],
      "metadata": {
        "id": "Szivt7qr79nm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1ol0fYR9-X2",
        "outputId": "eb1303cb-33b6-41de-bb1f-d789c5dd59b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlmR3h9pa-ra",
        "outputId": "e8411adb-425e-46bf-a055-bc10d2e6101e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hazm\n",
            "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
            "\u001b[K     |████████████████████████████████| 316 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1\n",
            "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 48.1 MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 46.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394486 sha256=bd2f63d9ee358f45f1eb85a6af95be4e4b55c56773474fdc0fa3c450dce4668a\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/fd/0c/d92302c876e5de87ebd7fc0979d82edb93e2d8d768bf71fac4\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=153943 sha256=58565ddc8cd821795a3bd20e9a3abc85bdb0c03c99c2eec23cc2a46b58f74629\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/b2/5b/0fe4b8f5c0e65341e8ea7bb3f4a6ebabfe8b1ac31322392dbf\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, hazm\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   654  100   654    0     0   3780      0 --:--:-- --:--:-- --:--:--  3780\n",
            "100 29.1M  100 29.1M    0     0  49.0M      0 --:--:-- --:--:-- --:--:--  112M\n",
            "Archive:  ../resources-0.5.zip\n",
            "  inflating: chunker.model           \n",
            "  inflating: langModel.mco           \n",
            "   creating: lib/\n",
            "  inflating: lib/liblinear-1.8.jar   \n",
            "  inflating: lib/libsvm.jar          \n",
            "  inflating: lib/log4j.jar           \n",
            "  inflating: malt.jar                \n",
            "  inflating: postagger.model         \n",
            "--2022-01-25 10:14:36--  https://raw.githubusercontent.com/ashalogic/Persian-Sentiment-Analyzer/master/Tutorial_Dataset.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1444954 (1.4M) [text/plain]\n",
            "Saving to: ‘Tutorial_Dataset.csv’\n",
            "\n",
            "Tutorial_Dataset.cs 100%[===================>]   1.38M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2022-01-25 10:14:36 (26.1 MB/s) - ‘Tutorial_Dataset.csv’ saved [1444954/1444954]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install hazm\n",
        "!test -f resources-0.5.zip || curl -LO https://github.com/sobhe/hazm/releases/download/v0.5/resources-0.5.zip\n",
        "!test -d resources || ( mkdir -p resources && cd resources && unzip ../resources-0.5.zip )\n",
        "\n",
        "import hazm\n",
        "from __future__ import unicode_literals\n",
        "from hazm import *\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "#dataset\n",
        "!wget https://raw.githubusercontent.com/ashalogic/Persian-Sentiment-Analyzer/master/Tutorial_Dataset.csv\n",
        "csv_dataset = pd.read_csv(\"/content/Tutorial_Dataset.csv\")\n",
        "\n",
        "#lexicon\n",
        "################## Saved address of the downloaded lexicon file #####################\n",
        "\n",
        "lexicon = pd.read_excel(\"/content/drive/MyDrive/PerSent.xlsx\")\n",
        "\n",
        "################### pd.read_excel(\"Saved address of the downloaded lexicon file\") ###"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### preprocessing data"
      ],
      "metadata": {
        "id": "2l_9ZHkLzLQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CleanPersianText(text):\n",
        "  _normalizer = hazm.Normalizer()\n",
        "  text = _normalizer.normalize(text)\n",
        "  return text\n",
        "\n",
        "revlist = list(map(lambda x: [CleanPersianText(x[0]),x[1]],zip(csv_dataset['Text'],csv_dataset['Suggestion'])))\n",
        "pos=list(filter(lambda x: x[1] == 1,revlist))\n",
        "nat=list(filter(lambda x: x[1] == 2,revlist))\n",
        "neg=list(filter(lambda x: x[1] == 3,revlist))\n",
        "\n",
        "revlist_shuffle = pos[:450] + neg[:450]\n",
        "random.shuffle(revlist_shuffle)\n",
        "\n",
        "print(\"Posetive count {}\".format(len(pos)))\n",
        "print(\"Negetive count {}\".format(len(neg)))\n",
        "print(\"Natural  count {}\".format(len(nat)))\n",
        "print()\n",
        "print(\"Total    count {}\".format(len(revlist)))\n",
        "print()\n",
        "print(\"Posetive count : \",\"\\n\",pos[random.randrange(1,len(pos))])\n",
        "print(\"Negetive count : \",\"\\n\",neg[random.randrange(1,len(neg))])\n",
        "print(\"unknown  count : \",\"\\n\",nat[random.randrange(1,len(nat))])\n",
        "print(\"Total    count {}\".format(len(revlist_shuffle)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1L7mNInzFgo",
        "outputId": "4cb2ae1a-bc32-46d9-9e29-10084c55b681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Posetive count 2382\n",
            "Negetive count 460\n",
            "Natural  count 419\n",
            "\n",
            "Total    count 3261\n",
            "\n",
            "Posetive count :  \n",
            " ['همیشه عالی ', 1]\n",
            "Negetive count :  \n",
            " ['دوستان در خریدش شک نکنید اندازه کوچک و قدرت بالا عالیه ', 3]\n",
            "unknown  count :  \n",
            " ['این در باز کن فقط برای باز کردن درب بطری نوشیدنی کاربرد داره و واسه استفاده خانگی ضروری به نظر نمیاد بهتر بود که امکان باز کردن قوطی و کنسرو رو بهش اضافه میکردن ', 2]\n",
            "Total    count 900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### just lexicon"
      ],
      "metadata": {
        "id": "9D-v9dKeydfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statistics import *\n",
        "\n",
        "lemmatizer = Lemmatizer()\n",
        "tagger = POSTagger(model='resources/postagger.model')\n",
        "\n",
        "li_key = []\n",
        "li_value = []\n",
        "import statistics\n",
        " \n",
        "m = []\n",
        "t = []\n",
        "for i in range(len(revlist_shuffle[:])):\n",
        "  text = revlist_shuffle[i][0]\n",
        "  tag = tagger.tag(word_tokenize(text))\n",
        "  lem_tag = [] \n",
        "  value_polarity = []\n",
        "  for x in tag:\n",
        "    lem_tag.append(lemmatizer.lemmatize(x[0]))\n",
        "  idx =[]\n",
        "  for lem in tag:\n",
        "    if lem[0] in lexicon['Words'].tolist():\n",
        "      idx.append(lexicon['Words'].tolist().index(lem[0]))\n",
        "  value = []\n",
        "  if len(idx) > 0:\n",
        "    for x in idx:\n",
        "        value.append(lexicon['Polarity'].tolist()[x])\n",
        "  else:\n",
        "    value.append(1)\n",
        "  if statistics.mean(value) < 0:\n",
        "    m.append(0)\n",
        "  else:\n",
        "    m.append(1)\n",
        "  t.append(revlist_shuffle[i][1])\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(t, m))\n",
        "print(accuracy_score(t, m, normalize=False))"
      ],
      "metadata": {
        "id": "fYCHf235xd5o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ff8ed07-34b3-493c-c376-4f0b8786b886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4255555555555556\n",
            "383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### rules and lexicon"
      ],
      "metadata": {
        "id": "TXtFwImYxY_J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImEuHi3Ehc8p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85eb90ba-a41f-4ece-e11e-f6eae0371e20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5155555555555555\n",
            "464\n"
          ]
        }
      ],
      "source": [
        "#article rule => https://www.sciencedirect.com/science/article/abs/pii/S0925231219313815\n",
        "from statistics import *\n",
        "\n",
        "lemmatizer = Lemmatizer()\n",
        "tagger = POSTagger(model='resources/postagger.model')\n",
        "\n",
        "li_key = []\n",
        "li_value = []\n",
        "keyword_list = ['اما' , 'اگرچه' , 'با اینکه', 'ولی']\n",
        "keyword_list2 = ['حالیکه', 'صورتیکه']\n",
        "\n",
        "for i in range(len(revlist_shuffle)):\n",
        "  text = revlist_shuffle[i][0]\n",
        "  tag = tagger.tag(word_tokenize(text))\n",
        "  # rule 2  -----------------------------------------------------------------------------  rule 2 \n",
        "  value_polarity = []\n",
        "  if 'که' in text:\n",
        "    #print(text.split('که')[0])\n",
        "    target = text.split('که')[0]\n",
        "    tag_target = tagger.tag(word_tokenize(target))\n",
        "    #print(tag_target)\n",
        "    lem_target_tag = [] \n",
        "    for x in tag_target:\n",
        "      lem_target_tag.append(lemmatizer.lemmatize(x[0]))\n",
        "    #print('lem_target_tag',lem_target_tag)\n",
        "    for lem in lem_target_tag:\n",
        "      #به ازای هر کلمه ای که در لکسیکون داریم\n",
        "      df_have = lexicon.loc[lexicon['Words'] == lem]\n",
        "      #print('df_have', df_have)\n",
        "      #print('len df_have', len(df_have))\n",
        "      if len(df_have) > 1:\n",
        "        #print('>1')\n",
        "        value_have = df_have[\"Polarity\"].mean()\n",
        "        \n",
        "        value_polarity.append(value_have)\n",
        "      elif len(df_have) == 1:\n",
        "        #print('=1')\n",
        "        value_have = df_have['Polarity']\n",
        "        if value_have.values[0] != 0:\n",
        "          value_polarity.append(value_have.values[0])\n",
        "    #print(value_polarity)\n",
        "    if len(value_polarity) > 0:\n",
        "      \n",
        "      li_key.append(i)\n",
        "      li_value.append(statistics.mean(value_polarity))\n",
        "# rule 2  -----------------------------------------------------------------------------  rule 2 \n",
        "\n",
        "\n",
        "# rule 7  -----------------------------------------------------------------------------  rule 7 \n",
        "\n",
        "  if 'مخالف' in text: \n",
        "    \n",
        "    li_key.append(i)\n",
        "    li_value.append(-1)\n",
        "# rule 7  -----------------------------------------------------------------------------  rule 7 \n",
        " \n",
        "# rule 3  -----------------------------------------------------------------------------  rule 3 \n",
        "  value_polarity = []\n",
        "  if [key for key in keyword_list if key in text ]:\n",
        "    new_keyword_list = [key for key in keyword_list if key in text ]\n",
        "\n",
        "    value_polarity = []\n",
        "    #print(new_keyword_list,CONDITION)\n",
        "    for x in new_keyword_list:\n",
        "      value_polarity = []\n",
        "      #print(text.split('که')[0])\n",
        "      target = text.split(x)[1]\n",
        "      tag_target = tagger.tag(word_tokenize(target))\n",
        "      #print(tag_target)\n",
        "      lem_target_tag = [] \n",
        "      for x in tag_target:\n",
        "        lem_target_tag.append(lemmatizer.lemmatize(x[0]))\n",
        "      #print('lem_target_tag',lem_target_tag)\n",
        "      for lem in lem_target_tag:\n",
        "        #به ازای هر کلمه ای که در لکسیکون داریم\n",
        "        df_have = lexicon.loc[lexicon['Words'] == lem]\n",
        "        #print('df_have', df_have)\n",
        "        #print('len df_have', len(df_have))\n",
        "        if len(df_have) > 1:\n",
        "          #print('>1')\n",
        "          value_have = df_have[\"Polarity\"].mean()\n",
        "          \n",
        "          value_polarity.append(value_have)\n",
        "        elif len(df_have) == 1:\n",
        "          #print('=1')\n",
        "          value_have = df_have['Polarity']\n",
        "          if value_have.values[0] != 0:\n",
        "            value_polarity.append(value_have.values[0])\n",
        "\n",
        "    if len(value_polarity) > 0:\n",
        "      \n",
        "      li_key.append(i)\n",
        "      li_value.append(statistics.mean(value_polarity))\n",
        "# rule 3  -----------------------------------------------------------------------------  rule 3\n",
        "\n",
        "# rule 4  -----------------------------------------------------------------------------  rule 4 \n",
        "  value_polarity = []\n",
        "  if [key for key in keyword_list2 if key in text ]:\n",
        "    new_keyword_list2 = [key for key in keyword_list2 if key in text ]\n",
        "\n",
        "    value_polarity = []\n",
        "    #print(new_keyword_list2,CONDITION)\n",
        "    for x in new_keyword_list2:\n",
        "      value_polarity = []\n",
        "      #print(text.split('که')[0])\n",
        "      target = text.split(x)[1]\n",
        "      tag_target = tagger.tag(word_tokenize(target))\n",
        "      #print(tag_target)\n",
        "      lem_target_tag = [] \n",
        "      for x in tag_target:\n",
        "        lem_target_tag.append(lemmatizer.lemmatize(x[0]))\n",
        "      #print('lem_target_tag',lem_target_tag)\n",
        "      for lem in lem_target_tag:\n",
        "        #به ازای هر کلمه ای که در لکسیکون داریم\n",
        "        df_have = lexicon.loc[lexicon['Words'] == lem]\n",
        "        #print('df_have', df_have)\n",
        "        #print('len df_have', len(df_have))\n",
        "        if len(df_have) > 1:\n",
        "          #print('>1')\n",
        "          value_have = df_have[\"Polarity\"].mean()\n",
        "          \n",
        "          value_polarity.append(value_have)\n",
        "        elif len(df_have) == 1:\n",
        "          #print('=1')\n",
        "          value_have = df_have['Polarity']\n",
        "          if value_have.values[0] != 0:\n",
        "            value_polarity.append(value_have.values[0])\n",
        "\n",
        "    if len(value_polarity) > 0:\n",
        "      \n",
        "      li_key.append(i)\n",
        "      li_value.append(statistics.mean(value_polarity))\n",
        "# rule 4  -----------------------------------------------------------------------------  rule 4\n",
        "\n",
        "# rule 5  -----------------------------------------------------------------------------  rule 5 \n",
        "  if [x for x in tag if 'که' in x ]:\n",
        "    index_list = [tag.index(x) for x in tag if 'که' in x ]\n",
        "    value_polarity = []\n",
        "    idx = index_list[0]\n",
        "    if len(tag)> idx+1 and'این' in tag[idx+1]:\n",
        "      #print(text.split('این')[1])\n",
        "      target = text.split('این')[1]\n",
        "      value_polarity = []\n",
        "      #print(text.split('که')[0])\n",
        "      tag_target = tagger.tag(word_tokenize(target))\n",
        "      #print(tag_target)\n",
        "      lem_target_tag = [] \n",
        "      for x in tag_target:\n",
        "        lem_target_tag.append(lemmatizer.lemmatize(x[0]))\n",
        "      #print('lem_target_tag',lem_target_tag)\n",
        "      for lem in lem_target_tag:\n",
        "        #به ازای هر کلمه ای که در لکسیکون داریم\n",
        "        df_have = lexicon.loc[lexicon['Words'] == lem]\n",
        "        #print('df_have', df_have)\n",
        "        #print('len df_have', len(df_have))\n",
        "        if len(df_have) > 1:\n",
        "          #print('>1')\n",
        "          value_have = df_have[\"Polarity\"].mean()\n",
        "          \n",
        "          value_polarity.append(value_have)\n",
        "        elif len(df_have) == 1:\n",
        "          #print('=1')\n",
        "          value_have = df_have['Polarity']\n",
        "          if value_have.values[0] != 0:\n",
        "            value_polarity.append(value_have.values[0])\n",
        "      \n",
        "    if len(value_polarity) > 0:\n",
        "      \n",
        "      li_key.append(i)\n",
        "      li_value.append(statistics.mean(value_polarity))\n",
        "  \n",
        "  # rule 5  -----------------------------------------------------------------------------  rule 5\n",
        "\n",
        "  # rule 9  -----------------------------------------------------------------------------  rule 9\n",
        "  if [x for x in tag if x[1] == 'AJ' ]:\n",
        "    word = [x[0] for x in tag if x[1] == 'AJ' ]\n",
        "    #print(word)\n",
        "    negative_word = [x for x in word if x.startswith(\"نا\") or x.startswith(\"بی\") or x.startswith(\"ضد\") or x.startswith(\"زهر\") or x.startswith(\"لا\")]\n",
        "    #print(negative_word)\n",
        "    z = [x for x in negative_word if 'بیش' not in x]\n",
        "\n",
        "    for x in z:\n",
        "      if len(x) > 1:\n",
        "        \n",
        "        li_key.append(i)\n",
        "        li_value.append(-1)\n",
        "    #print(i)\n",
        "    #if word[0].startswith(\"نا\") or word[0].startswith(\"بی\") or word[0].startswith(\"ضد\") or word[0].startswith(\"زهر\") or word[0].startswith(\"لا\"):\n",
        "      #if word[0] != 'بیش' and word[0] != 'بیشتر' and word[0] != 'لازم' and word[0] != 'بیشتری' and word[0] != 'لایک' :\n",
        "        #print(x,lemmatizer.lemmatize(x[0]))\n",
        "        #d[i] = -1\n",
        "        #li_key.append(i)\n",
        "        #li_value.append(-1)\n",
        "  # rule 9  -----------------------------------------------------------------------------  rule 9\n",
        "  \n",
        "\n",
        "\n",
        "#___________________________________________________________________________________________________________________________________________#\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "df = pd.DataFrame({'li_key':li_key , 'li_value':li_value})\n",
        "df_gg = df.loc[df.groupby('li_key')['li_value'].apply(lambda x: x.abs().idxmax())]\n",
        "df_gg.loc[df_gg.li_value < 0, 'li_value'] = 0\n",
        "df_gg.loc[df_gg.li_value > 0, 'li_value'] = 1\n",
        "\n",
        "true_value_rule = []\n",
        "\n",
        "for i in df_gg['li_key'].tolist(): \n",
        "  true_value_rule.append(revlist_shuffle[i][1])\n",
        "\n",
        "none_polarity_index = []\n",
        "for i in range(len(revlist_shuffle)):    \n",
        "  if i not in df_gg['li_key'].tolist():\n",
        "    none_polarity_index.append(i)\n",
        "\n",
        "true_value_non_polarity = []\n",
        "for i in none_polarity_index:\n",
        "  true_value_non_polarity.append(revlist_shuffle[i][1]) \n",
        "\n",
        "positive_value_of_non_polarity= [1] * len(none_polarity_index)\n",
        "\n",
        "\n",
        "df_gg['true_value'] = true_value_rule\n",
        "\n",
        "df_non_polarity = pd.DataFrame(\n",
        "    {\n",
        "      'li_key': none_polarity_index,\n",
        "     'li_value': positive_value_of_non_polarity,\n",
        "     'true_value' : true_value_non_polarity\n",
        "    })\n",
        "\n",
        "frames = [df_gg, df_non_polarity]\n",
        "result = pd.concat(frames)\n",
        "result.loc[result.true_value == 3, 'true_value'] = 0\n",
        "result.loc[result.true_value == 1, 'true_value'] = 1\n",
        "\n",
        "\n",
        "y_true = result['true_value'].tolist()\n",
        "y_pred = result['li_value'].tolist()\n",
        "print(accuracy_score(y_true, y_pred))\n",
        "print(accuracy_score(y_true, y_pred, normalize=False))\n",
        "     "
      ]
    }
  ]
}